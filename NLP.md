# Paper for NLP

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

### General 
http://yagami12.hatenablog.com/entry/2017/12/30/175113
http://web.stanford.edu/class/cs224n/syllabus.html
 
## Word to vec
http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model<br>
http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf<br>
https://arxiv.org/pdf/1301.3781.pdf<br>
https://nlp.stanford.edu/pubs/glove.pdf<br>
http://www.aclweb.org/anthology/Q15-1016<br>
http://www.aclweb.org/anthology/D15-1036<br>

## Recurrent Neural Networks and Language Models
https://web.stanford.edu/~jurafsky/slp3/4.pdf<br>
 
## Machine Translation, Seq2Seq and Attention 
https://www.aclweb.org/anthology/P02-1040.pdf<br>
https://arxiv.org/pdf/1409.3215.pdf<br>
https://arxiv.org/pdf/1409.0473.pdf<br>
https://distill.pub/2016/augmented-rnns/<br>
https://arxiv.org/pdf/1703.03906.pdf<br>

## Advanced Attention 
https://arxiv.org/abs/1705.04304<br>
https://arxiv.org/abs/1704.04368<br>
https://arxiv.org/abs/1511.06909<br>
https://arxiv.org/abs/1604.00788<br>
https://arxiv.org/pdf/1611.01576.pdf<br>

## Transformer Networks and CNNs
https://arxiv.org/abs/1706.03762<br>
https://arxiv.org/pdf/1607.06450.pdf<br>
https://arxiv.org/abs/1408.5882<br>
https://arxiv.org/abs/1207.0580<br>
https://arxiv.org/pdf/1404.2188.pdf<br>

